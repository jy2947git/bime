							Scale in Cloud

In the first architecture article, we talked about bime and especially how we re-factor bime to be able to serve multiple
clients(labs) while still maintaining separated lab database.

we will have one server to host the "Bime Controller" and N server to host "Bime" application. So, we need to think through how the 2 applications communicate with each other
in a cluster environment.

There are different communication types between distributed -
1, synchronous vs asynchronous
Synchronous communication is usually simple by demands high level of latency, for example, RPC, web service (SOAP or Restful).
Asynchronous communication is good for throughput because the requester doesn't have to wait for response. "Queue" is usually involved in asynchronous communications.

2, request only vs request-reply
In general, request-reply provide better reliability, and of course more complicated.

In our case, the Bime-Controller needs to 
1)let the bime server instance know to prepare the datasource etc for the new client
2)know when the (1) is done so it will send response to user (web or email)

Other factors to consider: is the operation a short or a long one? What kind of user experience do you want to provide?

The last question is critical because eventually the user experience is the most important thing for a web application (or any business).
In the case of Bime, our answers are:
1, use experience wise, we "really" want the user "immediately" gets the message that "server is ready, please use it now". So, we'd prefer
to have a immediate response. Alternatively, it is acceptable to notify users later through email.
2, it is a short operation if we have a existing Bime server and we only need to add new data source for client; it will be much longer (in minutes) if
it involves creating a new server (when all existing bime server has full capacity).

So, our design is:
1, when the Bime Controller get request to set up for new client, it will first do an estimate - check its internal instance-status table to see whether there
is existing server which can host the new client. If found, we will sought to immediate reply-response model; otherwise, utilize asynchronous mode, display thank you
message and notify user through email when server is ready.
2, In the immediate request-response mode, the Bime Controller knows which Bime server has the capacity, it will contact it directly to ask to prepare the new client, and
wait until it is done; So, we will implement a Restful web service at Bime application. The Bime Controller (as the service client) will send a POST request to Bime, and Bime
return response after new client is set up for use.
3, In the asynchronous mode, the flow is more complicated:
(1)create the new instance (which can take minutes and request only operation)
(2)create new client on the new instance
(3)send notification email.
Also, in the scenario of N clients requested at same time to Bime Controller, we dont want to create N new server instances; we'd only want to create 1 new instance, and then create
N clients on it......it is a concurrency issue. Similarly if we have N clients requests, and one Bime instance has capacity to host new clients. In the multi-threading environment,
we may ask the Bime instance to set up for N new clients, which is beyond its limit.

Now we are more of talking of how to scale in cloud environment - our approach is
1, there will be a Task Manager within the Bime Controller web application.  Task Manager creates and manages a small internal queue (called Engagement Queue) for EVERY existing worker (Bime) instance; The queue
is to hold the "task", in our case, the client being served by this particular Bime EC2 instance. Bime Controller exposes a RESTful interface for Bime instance to report its status.
2, When Bime instance starts up, it will make a Restful POST call to Bime Controller to update its status - name, IP, the names of the clients; and task manager will update its engagement queue.
3, When a new client signed up, Bime Controller will first consult with the Task Manager: please add a new client ABC, and how long will it take? 
4, Task manager will check all the engagement queues to see which worker has spare capacity - if found, it will immediately put a task into the engagement queue (synchronized); tell Bime Controller that "Got it, working on it, it will be soon". Task manager then make a Restful 
POST call to the particular worker instance to ask it to check its queue; Bime check Bime will prepare for the new client and make a call to the call-back URL once it is done; Task Manager then inform Bime Controller that the work is done.
5, If no worker has more spare catacity, then a new worker needs to be created; First, task manager will notify Bime Controller that "it will take a while, so why not just return message to user and we will notify him through email later"; Task manager then create a new engagement queue for the future new instance, and put the request into the queue; it then will issue command to
create a new worker server instance (ie, EC2); This will take a while, so Task manager will return immediately.
6, Once the new worker instnace starts, it will 



First, we need to re-configure the Haproxy to tell it the mapping between "lab30.bime.com" to "sever9". That means, we need a small application on the same server with Haproxy.
It will listen to a queue "ReconfigureForNewClient" and update the configuration file with messages sent from Bime-Controller.
Second, In above example, we were assuming there is areadly a Bime server running and we were only adding data source. What if all the existing servers have full compacity (remeber
we plan to have 20 clients per server)? In this case we will need a new server setup and running first. One simple solution is to always have couple of spare instances running;
However, in the scenario of the SaaS server is very successful and clients are registering faster than you manually create servers, we need some automatic scaling solutions.
Assume we are using the Amazon Web Service cloud, through which we can relatively create new instances on the fly.

The flow of events will be
1, start Hornetq server "192.168.2.2"
2, start Bime-Controller server "192.168.2.3"
3, start Haproxy server, which only has one routing rule "www.bime.com -> 192.168.2.3"
On the queue server, there is queue "WorkerInstanceStatus", for Bime servers to report their status to Bime-Controller, thus Bime-Controller can have knowlege of "who is alive and serving
which client", its own routing table.
Right now there is nothing in the table, and nothing in the queue.
First client register its lab
4, Bime Controller checks its routing table, to find a suitable server instance which has space to add new client. In this case, exists.
5, Bime Controller decides first a new server is needed; it will put a message into a special queue "CreateNewBimeInstance"
5, also, Bime Controllerw will put the pending client to a pending client queue - there is no listener on this one though.
5, then Bime Controller return the web. At front end, there is javascript to keep pooling url http://www.bime.com/status/lab444
6, There is a system application listening to this queue, and will execute a script to create a new EC2 instance.
In step 6, the AMI to create the EC2 instance already has all the necessary OS(Linux Ubuntu), packages(Java, tomcat, mysql etc) and Application(Bime) installed and configured.
7, The new Bime instance will send the status message to the queue (it already knows the queue server ip) about its name and clients.
8, Bime-Contrller listener gets the status message and update the routing table 
9, If Bime-Controller notices any instance has 0 client - it will check whether there is any clients in the pending client queue.
  